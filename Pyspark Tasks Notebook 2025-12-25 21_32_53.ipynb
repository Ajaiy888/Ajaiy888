{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ffa0e52-a6fa-43f2-9dfa-0109b00c49f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1,.create a pyspark application\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca3b24a8-e3b1-4bf3-923f-8f1cc7f1781a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2.import the necessary types as classes\n",
    "from pyspark.sql.types import (StructType, StructField,IntegerType,StringType,ArrayType)\n",
    "#3.Construct the schema\n",
    "schema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                     StructField(\"name\", StringType(), True),\n",
    "                     StructField(\"scores\", IntegerType(), True),\n",
    "            ])\n",
    "# Define the data as a list of tuples\n",
    "data = [\n",
    "    (1, \"Alice\", 85),\n",
    "    (2, \"Bob\", 90),\n",
    "    (3, \"Charlie\", 78)\n",
    "]\n",
    "#4.Set the schema\n",
    "df = spark.createDataFrame(data,schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a7a1c15-c550-4cdc-bd6d-0d43fce93306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507e515d-c261-42a3-a8da-df7837673efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"age\",IntegerType()),\n",
    " StructField(\"education_num\",IntegerType()),\n",
    " StructField(\"marital_status\",StringType()),\n",
    " StructField(\"occupation\",StringType()),\n",
    " StructField(\"income\",StringType()),\n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb824703-0292-478c-92bd-895ab7c3d9bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "census_adult = spark.read.csv(\"dbfs:/path/to/your/file/adult_reduced_100.csv\",sep=',', header=False,\n",
    "schema=schema)\n",
    "census_adult.show()\n",
    "census_adult.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35fce150-2ca3-43a6-9fd8-58e5a92fe1f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#sort using  the age column\n",
    "df.sort(\"age\",ascending =False).show()\n",
    "# Drop missing values\n",
    "df.na.drop().show()\n",
    "# fill missing values\n",
    "df.na.fill(0).show()\n",
    "df.na.fill(\"unknown\").show()\n",
    "# Replace missing values with a specific value\n",
    "df.na.fill(0).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "783d948a-0724-409e-acb5-4c46018350d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#drop rows with any nulls\n",
    "df_cleaned = df.na.drop()\n",
    "#filter out nulls\n",
    "df_cleaned = df.where(col(\"age\")).isNotNull())\n",
    "#replace nulls with a specific value\n",
    "df_cleaned = df.na.fill(0)\n",
    "#fill nulls in the age column with the value 0\n",
    "df_filled = df.na.fill({\"age\":0})\n",
    "df_filled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7b42f01-237a-4296-b98f-c450cb95f4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#creating a new column\n",
    "df = df.withColumn(\"age_group\", when(col(\"age\") < 30, \"young\").otherwise(\"old\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5612615-0370-4837-ae13-31458033cacd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#renaming columns\n",
    "df = df.withColumnRenamed(\"age\", \"age_new\")\n",
    "df.show()\n",
    "#dropping columns\n",
    "df = df.drop(\"age\")\n",
    "df.show()\n",
    "#selecting columns\n",
    "df.select(\"age\", \"name\").show()\n",
    "#selecting rows\n",
    "df.filter(col(\"age\") > 30).show()\n",
    "#selecting rows and columns\n",
    "df.filter(col(\"age\") > 30).select(\"age\", \"name\").show()\n",
    "#group by\n",
    "df.groupBy(\"age\").count().show()\n",
    "#group by and aggregate\n",
    "df.groupBy(\"age\").agg(avg(\"age\")).show()\n",
    "#join\n",
    "df.join(df2, df.id == df2.id, \"inner\").show()  \n",
    "#save the dataframe as a csv file\n",
    "df.write.csv(\"dbfs:/path/to/your/file/output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6658f505-91d4-4479-b929-d0abb4085dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#filtering rows\n",
    "df.filter(col(\"age\") > 30).show()\n",
    "#selecting columns\n",
    "df.select(\"age\", \"name\").show()\n",
    "#grouping rows\n",
    "df.groupBy(\"age\").count().show()\n",
    "#joining two dataframes\n",
    "df.join(df2, df.id == df2.id, \"inner\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec53fce-6331-482c-98df-073b61ec9ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#grouping rows\n",
    "df.groupBy(\"category\").agg(avg(\"price\")).show()\n",
    "df_grouped = df.groupby(\"category\").agg({\"value_column\": \"avg\"})\n",
    "df_grouped.show()\n",
    "df_grouped =df.groupby(\"category\").agg({\"value_column\": \"avg\"})\n",
    "df_grouped.show()\n",
    "#filtering rows\n",
    "df.filter(col(\"age\") > 30).show()\n",
    "'#selecting columns\n",
    "df.select(\"age\", \"name\").show()\n",
    "#joining two dataframes\n",
    "df.join(df2, df.id == df2.id, \"inner\").show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f1949db-9beb-4bd0-9788-fdd252f88a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_union = df1.union(df2)\n",
    "df_union.show()\n",
    "df = df.withColumn(\"age_group\", when(col(\"age\") < 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8c23fed-41b8-4b99-bd0c-1fe56146d41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#working with arrays and maps\n",
    "from pyspark.sql.functions import explode, split, array_contains, map_keys, map_values, map_entries, map_from_arrays, array_sort, array_distinct, array_remove, array_union, array_intersect, array_except, array_zip, array    concat, array_repeat, array_position, array_sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c4a501e-477c-400b-ad71-019aa9bdf160",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df =df.withColumn(\"array_column\",lit([1,2,3]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431d762d-f732-4eb6-aa07-6cd7972643ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"map_column\",lit({\"key1\":\"value1\",\"key2\":\"value2\"}))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c73249e0-90f1-4d7f-a293-c2dd867f4c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "@pandas_udf(\"float\")\n",
    "def calculate_squares(x: pd.Series)\") -> pd.Series:\n",
    "    return x**2\n",
    "df = df.withColumn(\"squares\",calculate_squares(df[\"array_column\"]))\n",
    "df.show()\n",
    "df = df.withColumn(\"squares\",calculate_squares(df[\"array_column\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c1167f-d409-4d1b-aa85-56498c9f3b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "@pandas_udf(\"float\")\n",
    "def calculate_squares(x: pd.Series)\") -> pd.Series:\n",
    "    return x**2\n",
    "df = df.withColumn(\"squares\",calculate_squares(df[\"array_column\"]))\n",
    "df.show()\n",
    "df = df.withColumn(\"squares\",calculate_squares(df[\"array_column\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7175d1b3-8396-4d37-99f2-3044348103ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fahrenheit_to_celsius(fahrenheit):\n",
    "    return (fahrenheit - 32) * 5 / 9\n",
    "df = df.withColumn(\"celsius\",fahrenheit_to_celsius(df[\"temperature\"]))\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark Tasks Notebook 2025-12-25 21:32:53",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}